# KAN-LLaMA: An Interpretable Large Language Model With KAN-based Sparse Autoencoders

## CSE 5525 Final Project @ The Ohio State University

## Authors: Alex Gulko, Yusen Peng

## Project check-in task-list

### preliminary exploration

1. actually train an SAE on Llama 3.2 1B (prototype done)
    now we are able to train a SAE upon Llama 3.2 1B with the dataset "apollo-research/roneneldan-TinyStories-tokenizer-gpt2" using ONLY 1M tokens;
2. analyze the trained SAE for Llama 3.2 1B: L0 test (prototype done)
    now we are able to extract average L0 scores for every single batch and put them into a CSV file;

3. expressiveness evaluation: reconstruction test + zero ablation test


4. expressiveness evaluation: specific capability test


5. interpretability features: Logits Lens



6. interpretability features: SAE feature dashboard







## KAN integration

1. KAN autoencoder code
2. 
