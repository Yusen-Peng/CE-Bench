sae.py:332 self.cfg.activation_fn_kwargs={'kan_hidden_size': 2048, 'kan_ae_type': 'kan_relu_dense'}
line 1181: hidden_size set to bottleneck_size: 2048
Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer
Tokenized prompt: ['<|begin_of_text|>', 'If', ' the', ' glass', ' falls', ' off', ' the', ' table', ',', ' it', ' will']
Tokenized answer: [' break']
Performance on answer token:
Rank: 2        Logit: 20.85 Prob: 12.82% Token: | break|
Top 0th token. Logit: 20.90 Prob: 13.47% Token: | be|
Top 1th token. Logit: 20.87 Prob: 13.05% Token: | not|
Top 2th token. Logit: 20.85 Prob: 12.82% Token: | break|
Top 3th token. Logit: 20.04 Prob:  5.69% Token: | fall|
Top 4th token. Logit: 19.89 Prob:  4.89% Token: | hit|
Top 5th token. Logit: 18.78 Prob:  1.61% Token: | sh|
Top 6th token. Logit: 18.72 Prob:  1.52% Token: | cause|
Top 7th token. Logit: 18.59 Prob:  1.34% Token: | make|
Top 8th token. Logit: 18.59 Prob:  1.33% Token: | have|
Top 9th token. Logit: 18.52 Prob:  1.25% Token: | probably|
Ranks of the answer tokens: [(' break', 2)]
Tokenized prompt: ['<|begin_of_text|>', 'If', ' the', ' glass', ' falls', ' off', ' the', ' table', ',', ' it', ' will']
Tokenized answer: [' break']
Padded sae_out from torch.Size([1, 10, 2048]) to torch.Size([1, 12, 2048])
Performance on answer token:
Rank: 242      Logit: 12.62 Prob:  0.04% Token: | break|
Top 0th token. Logit: 18.21 Prob: 11.66% Token: |,|
Top 1th token. Logit: 17.57 Prob:  6.17% Token: |
|
Top 2th token. Logit: 17.37 Prob:  5.07% Token: | be|
Top 3th token. Logit: 16.82 Prob:  2.91% Token: | if|
Top 4th token. Logit: 16.42 Prob:  1.96% Token: | =|
Top 5th token. Logit: 16.38 Prob:  1.88% Token: | t|
Top 6th token. Logit: 16.34 Prob:  1.81% Token: | not|
Top 7th token. Logit: 16.27 Prob:  1.68% Token: | it|
Top 8th token. Logit: 16.14 Prob:  1.47% Token: |<|end_of_text|>|
Top 9th token. Logit: 16.10 Prob:  1.42% Token: | how|
Ranks of the answer tokens: [(' break', 242)]
