sae.py:332 self.cfg.activation_fn_kwargs={'kan_hidden_size': 16384, 'kan_ae_type': 'only_kan'}
Loaded pretrained model gpt2-small into HookedTransformer
Tokenized prompt: ['<|endoftext|>', 'If', ' the', ' glass', ' falls', ' off', ' the', ' table', ',', ' it', ' will']
Tokenized answer: [' break']
Performance on answer token:
Rank: 3        Logit: -116.40 Prob:  4.61% Token: | break|
Top 0th token. Logit: -115.61 Prob: 10.18% Token: | fall|
Top 1th token. Logit: -115.76 Prob:  8.81% Token: | be|
Top 2th token. Logit: -116.32 Prob:  5.02% Token: | not|
Top 3th token. Logit: -116.40 Prob:  4.61% Token: | break|
Top 4th token. Logit: -116.54 Prob:  4.04% Token: | shatter|
Top 5th token. Logit: -117.67 Prob:  1.30% Token: | cause|
Top 6th token. Logit: -117.67 Prob:  1.30% Token: | go|
Top 7th token. Logit: -117.70 Prob:  1.26% Token: | have|
Top 8th token. Logit: -117.74 Prob:  1.22% Token: | explode|
Top 9th token. Logit: -117.85 Prob:  1.08% Token: | look|
Ranks of the answer tokens: [(' break', 3)]
Tokenized prompt: ['<|endoftext|>', 'If', ' the', ' glass', ' falls', ' off', ' the', ' table', ',', ' it', ' will']
Tokenized answer: [' break']
Padded sae_out from torch.Size([1, 10, 768]) to torch.Size([1, 12, 768])
Performance on answer token:
Rank: 3        Logit: -117.22 Prob:  4.33% Token: | break|
Top 0th token. Logit: -116.59 Prob:  8.14% Token: | be|
Top 1th token. Logit: -116.80 Prob:  6.57% Token: | fall|
Top 2th token. Logit: -116.91 Prob:  5.88% Token: | not|
Top 3th token. Logit: -117.22 Prob:  4.33% Token: | break|
Top 4th token. Logit: -117.59 Prob:  2.97% Token: | shatter|
Top 5th token. Logit: -118.21 Prob:  1.61% Token: | cause|
Top 6th token. Logit: -118.33 Prob:  1.42% Token: | have|
Top 7th token. Logit: -118.49 Prob:  1.21% Token: | become|
Top 8th token. Logit: -118.63 Prob:  1.05% Token: | go|
Top 9th token. Logit: -118.69 Prob:  1.00% Token: | explode|
Ranks of the answer tokens: [(' break', 3)]
