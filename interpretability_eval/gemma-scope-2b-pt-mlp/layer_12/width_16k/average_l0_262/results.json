{
    "sae_release": "gemma-scope-2b-pt-mlp",
    "sae_id": "layer_12/width_16k/average_l0_262",
    "contrastive_score_mean": 37.97270601016959,
    "independent_score_mean": 37.856331186196236,
    "interpretability_score_mean": 35.34622798801698,
    "total_rows": 500,
    "sae_config": {
        "architecture": "jumprelu",
        "d_in": 2304,
        "d_sae": 16384,
        "dtype": "torch.bfloat16",
        "device": "cuda",
        "model_name": "gemma-2-2b",
        "hook_name": "blocks.12.hook_mlp_out",
        "hook_layer": 12,
        "hook_head_index": null,
        "activation_fn_str": "relu",
        "activation_fn_kwargs": {},
        "apply_b_dec_to_input": false,
        "finetuning_scaling_factor": false,
        "sae_lens_training_version": null,
        "prepend_bos": true,
        "dataset_path": "monology/pile-uncopyrighted",
        "dataset_trust_remote_code": true,
        "context_size": 1024,
        "normalize_activations": null,
        "neuronpedia_id": null,
        "model_from_pretrained_kwargs": {},
        "seqpos_slice": [
            null
        ]
    },
    "date": "2025-05-07 03:25:37"
}