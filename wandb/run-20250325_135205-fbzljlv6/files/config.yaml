_wandb:
    value:
        cli_version: 0.19.8
        m: []
        python_version: 3.10.16
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 55
                - 71
                - 105
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 55
                - 71
                - 105
            "3":
                - 2
                - 13
                - 16
                - 23
                - 55
                - 61
            "4": 3.10.16
            "5": 0.19.8
            "6": 4.49.0
            "8":
                - 5
            "12": 0.19.8
            "13": linux-x86_64
act_store_device:
    value: cuda
activation_fn:
    value: relu
activation_fn_kwargs:
    value:
        kan_ae_type: kan_relu_dense
        kan_hidden_size: 16384
adam_beta1:
    value: 0.9
adam_beta2:
    value: 0.999
apply_b_dec_to_input:
    value: false
architecture:
    value: kan
autocast:
    value: false
autocast_lm:
    value: false
b_dec_init_method:
    value: zeros
cached_activations_path:
    value: null
checkpoint_path:
    value: checkpoints/f8kw1cmg
compile_llm:
    value: false
compile_sae:
    value: false
context_size:
    value: 128
d_in:
    value: 2048
d_sae:
    value: 16384
dataset_path:
    value: GulkoA/TinyStories-tokenized-Llama-3.2
dataset_trust_remote_code:
    value: true
dead_feature_threshold:
    value: 0.0001
dead_feature_window:
    value: 1000
decoder_heuristic_init:
    value: true
decoder_orthogonal_init:
    value: false
device:
    value: cuda
dtype:
    value: float32
eval_batch_size_prompts:
    value: null
eval_every_n_wandb_logs:
    value: 5
exclude_special_tokens:
    value: false
expansion_factor:
    value: 8
feature_sampling_window:
    value: 1000
finetuning_method:
    value: null
finetuning_tokens:
    value: 0
from_pretrained_path:
    value: null
hook_eval:
    value: NOT_IN_USE
hook_head_index:
    value: null
hook_layer:
    value: 0
hook_name:
    value: blocks.0.hook_mlp_out
init_encoder_as_decoder_transpose:
    value: true
is_dataset_tokenized:
    value: true
jumprelu_bandwidth:
    value: 0.001
jumprelu_init_threshold:
    value: 0.001
l1_coefficient:
    value: 5
l1_warm_up_steps:
    value: 150
llm_compilation_mode:
    value: null
log_activations_store_to_wandb:
    value: false
log_optimizer_state_to_wandb:
    value: false
log_to_wandb:
    value: true
lp_norm:
    value: 1
lr:
    value: 1e-05
lr_decay_steps:
    value: 600
lr_end:
    value: 1.0000000000000002e-06
lr_scheduler_name:
    value: cosineannealing
lr_warm_up_steps:
    value: 150
model_class_name:
    value: HookedTransformer
model_from_pretrained_kwargs:
    value:
        center_writing_weights: false
model_name:
    value: meta-llama/Llama-3.2-1B
mse_loss_normalization:
    value: dense_batch
n_batches_in_buffer:
    value: 8
n_checkpoints:
    value: 5
n_eval_batches:
    value: 10
n_restart_cycles:
    value: 1
noise_scale:
    value: 0
normalize_activations:
    value: expected_average_only_in
normalize_sae_decoder:
    value: false
prepend_bos:
    value: true
resume:
    value: false
run_name:
    value: 16384-L1-5-LR-1e-05-Tokens-1.229e+07
sae_compilation_mode:
    value: null
sae_lens_training_version:
    value: 5.5.2
sae_lens_version:
    value: 5.5.2
scale_sparsity_penalty_by_decoder_norm:
    value: true
seed:
    value: 42
seqpos_slice:
    value:
        - null
store_batch_size_prompts:
    value: 4
streaming:
    value: true
tokens_per_buffer:
    value: 524288
train_batch_size_tokens:
    value: 512
training_tokens:
    value: 12288000
use_cached_activations:
    value: false
use_ghost_grads:
    value: false
verbose:
    value: true
wandb_entity:
    value: null
wandb_id:
    value: null
wandb_log_frequency:
    value: 30
wandb_project:
    value: sae_llama_3_2_1B
